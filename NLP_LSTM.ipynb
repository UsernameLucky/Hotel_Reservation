{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UsernameLucky/Machine_Learning-projects/blob/main/NLP_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHFQH-Hhn-4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "#from torchtext.datasets import LanguageModelingDataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "#from torchtext.data import Field, BucketIterator\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU1O-rmxoCl2",
        "outputId": "dd2a7684-26ad-4650-dfce-d86036a5a709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch torchtext\n"
      ],
      "metadata": {
        "id": "9oYk08H-r0sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the files and concatenate them"
      ],
      "metadata": {
        "id": "HBDxOkmez1o6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 4 different txt files from 4 of Chinua Achebe's books; A Man of the People, Anthills of the Savannah, Arrow of God and Things Fall Apart that we need to concatenate to one dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "XZnE7vNwwCBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/assignment_2_&_data'\n",
        "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
        "texts = []\n",
        "for file in files:\n",
        "    with open(os.path.join(data_path, file), 'r', encoding='latin-1') as f:\n",
        "       text = f.read()\n",
        "    texts.append(text)\n",
        "combined_text = ' '.join(texts)\n"
      ],
      "metadata": {
        "id": "vXL35Tt0oIhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the following code to check the first characters of the text."
      ],
      "metadata": {
        "id": "xeFyjrpLxn-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_text[:1000])  # Print the first 1000 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEE8cwsxt2Ce",
        "outputId": "578e6b4e-e8b1-416f-d0b4-91098b67611f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anthills of the Savannah\n",
            "\n",
            "\n",
            "Chinua Achebe\n",
            "\n",
            "\n",
            "\tPublished 1988\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ONE\n",
            "_First Witness--Christopher Oriko__\n",
            "\n",
            "'YOU'RE WASTING EVERYBODY'S TIME, Mr. Commissioner for Information. I will not go to Abazon. Finish! _Kabisa!__ Any other business?'\n",
            "\t'As Your Excellency wishes. But ...'\n",
            "\t'But me no buts, Mr. Oriko! The matter is closed, I said. How many times, for God's sake, am I expected to repeat it? Why do _you__ find it so difficult to swallow my ruling. On anything?'\n",
            "\t'I am sorry, Your Excellency. But I have no difficulty swallowing _and__ digesting your rulings.'\n",
            "\tFor a full minute or so the fury of his eyes lay on me. Briefly our eyes had been locked in combat. Then I had lowered mine to the shiny table-top in ceremonial capitulation. Long silence. But he was not appeased. Rather he was making the silence itself grow rapidly into its own kind of contest, like the eyewink duel of children. I conceded victory there as well. Without raising my eyes I said again: 'I am very sorry, Your Excellen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code shows the first characters of our concatenated dataset. This happens to be Anthills of the Savannah."
      ],
      "metadata": {
        "id": "PIxXaiI-ypwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the last 1000 characters\n",
        "print(combined_text[-1000:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U7hl4cmuURl",
        "outputId": "a8fa3339-847c-4d11-8ebc-4d23c97ead8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e joint winner of the first Commonwealth Poetry Prize. Of his novels, Arrow of God is winner of the New Statesman-Jock Campbell Award, and Anthills of the Savannah was a finalist for the 1987 Booker Prize in England.\n",
            "\tMr. Achebe has received numerous honours from around the world, including the Honorary Fellowship of the American Academy and Institute of Arts and Letters, as well as more than twenty honorary doctorates from universities in England, Scotland, the United States, Canada, and Nigeria. He is also the recipient of Nigeria's highest award for intellectual achievement, the Nigerian National Merit Award.\n",
            "\tAt present, Mr. Achebe lives with his wife in Annandale, New York, where they both teach at Bard College. They have four children.\n",
            "\n",
            "\n",
            "ALSO BY CHINUA ACHEBE \n",
            "Anthills of the Savannah \n",
            "Arrow of God \n",
            "Girls at War and Other Stories \n",
            "A Man of the People \n",
            "No Longer at Ease \n",
            "\n",
            "Nonfiction: \n",
            "Hopes and Impediments: Selected Essays \n",
            "The Trouble With Nigeria \n",
            "\n",
            "Poetry: \n",
            "Beware Soul Brother\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code shows the last 1000 characters of the combined dataset. When we check this, they belong to Things Fall Apart. This shows that Things Fall Apart was the last one to be concatenated."
      ],
      "metadata": {
        "id": "jmkcYqgCzcMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the combined text into sentences\n",
        "sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', combined_text)\n",
        "\n",
        "# Print the last 10 sentences\n",
        "for sentence in sentences[-10:]:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6dAJ7Sy15Js",
        "outputId": "43082910-cbdf-4e0f-abf3-216b9d943f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He was appointed Senior Research Fellow at the University of Nigeria, Nsukka, and began lecturing widely abroad.\n",
            "\tFrom 1972 to 1976, and again in 1987 to 1988, Mr. Achebe was Professor of English at the University of Massachusetts, Amherst, and also for one year at the University of Connecticut, Storrs.\n",
            "\tCited in the London Sunday Times as one of the \"1,000 Makers of the Twentieth Century\" for defining \"a modern African literature that was truly African\" and thereby making \"a major contribution to world literature,\" has published novels, short stories, essays, and children's books.\n",
            "His volume of poetry, Christmas in Biafra, written during the Biafran War, was the joint winner of the first Commonwealth Poetry Prize.\n",
            "Of his novels, Arrow of God is winner of the New Statesman-Jock Campbell Award, and Anthills of the Savannah was a finalist for the 1987 Booker Prize in England.\n",
            "\tMr. Achebe has received numerous honours from around the world, including the Honorary Fellowship of the American Academy and Institute of Arts and Letters, as well as more than twenty honorary doctorates from universities in England, Scotland, the United States, Canada, and Nigeria.\n",
            "He is also the recipient of Nigeria's highest award for intellectual achievement, the Nigerian National Merit Award.\n",
            "\tAt present, Mr. Achebe lives with his wife in Annandale, New York, where they both teach at Bard College.\n",
            "They have four children.\n",
            "\n",
            "\n",
            "ALSO BY CHINUA ACHEBE \n",
            "Anthills of the Savannah \n",
            "Arrow of God \n",
            "Girls at War and Other Stories \n",
            "A Man of the People \n",
            "No Longer at Ease \n",
            "\n",
            "Nonfiction: \n",
            "Hopes and Impediments: Selected Essays \n",
            "The Trouble With Nigeria \n",
            "\n",
            "Poetry: \n",
            "Beware Soul Brother\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last ten sentences."
      ],
      "metadata": {
        "id": "6_2H1dRGSA8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of characters\n",
        "num_characters = len(combined_text)\n",
        "\n",
        "# Number of words\n",
        "words = combined_text.split()\n",
        "num_words = len(words)\n",
        "num_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Os0n2I3WRO",
        "outputId": "5d8508de-ac75-456b-e6f5-e081a85c91cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275041"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the number of words in the combined text."
      ],
      "metadata": {
        "id": "q1E06ASNN29M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of characters\n",
        "num_characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOPNsj7-3lS6",
        "outputId": "674bbecd-fdba-4852-c03a-0e48aaf5d079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1493656"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "N-se76z-0wHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We clean and tokenize the text. This involves removing any special characters that are not useful for training a language model, such as punctuation and numbers, and then splitting the text into tokens (words) that the model can learn from."
      ],
      "metadata": {
        "id": "0O0Y7su54LsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Example preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    #stemmer = PorterStemmer()\n",
        "    #tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Preprocess the concatenated text data\n",
        "preprocessed_data = preprocess_text(combined_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gVWC9YUWDu8",
        "outputId": "35b76d9c-4bef-45a3-d7a2-c1a2ff69da92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYsQnFTfsgG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import re\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "import os\n",
        "import requests\n",
        "import io"
      ],
      "metadata": {
        "id": "jWHqZNYFd9PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text.\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
        "                  for sent in sent_tokenize(preprocessed_data)]"
      ],
      "metadata": {
        "id": "vIUF-NVSd9Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized_text[0]"
      ],
      "metadata": {
        "id": "Gs6evqkLM4Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tokenized text for 3-grams language modelling\n",
        "n = 3 # n gram\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
      ],
      "metadata": {
        "id": "_A7x-zb8M4VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training an tri-gram Model"
      ],
      "metadata": {
        "id": "9LDn8ExRlgm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize MLE model - MLE model, creates an empty vocabulary\n",
        "model = MLE(n)\n"
      ],
      "metadata": {
        "id": "m-A_IZ4tM4Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "model.fit(train_data, padded_sents)\n",
        "print(model.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKJBzr9ylq2k",
        "outputId": "4d683db6-9802-4baa-a6cb-eafdc5a5f3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 15206 items>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCKVTimKlvDV",
        "outputId": "074ba521-b10c-44d7-c740-b74a282cf922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15206"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model.vocab.lookup(tokenized_text[0]))"
      ],
      "metadata": {
        "id": "gPcGFQeIl4MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.vocab.lookup('antihills savannah chinua achebe published.'.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46V960MdnLr6",
        "outputId": "1741b473-50c4-4b59-cfb2-f49660a9168b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<UNK>', 'savannah', 'chinua', 'achebe', '<UNK>')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the model"
      ],
      "metadata": {
        "id": "jS34kyJtrTIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyhzPyuirGJQ",
        "outputId": "34b88fe1-4bbd-4898-9f2d-25993d9740cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 398778 ngrams>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'possible'\n",
        "print(\"number of bigrams for the phrase\", phrase, model.counts[phrase])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDqmtOizrZ4z",
        "outputId": "873b6a26-52f1-4ab9-c7a1-3df45f1d8e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of bigrams for the phrase possible 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'ask'\n",
        "print(\"number of bigrams for the phrase\", phrase, model.counts[phrase])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_9vWf_FwhHa",
        "outputId": "339c4739-80e1-4d1f-8082-ab577354653f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of bigrams for the phrase ask 171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of bigrames of information followed by question\n",
        "print(\"number of bigrams for the phrase\", model.counts[['ask']]['question'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmNPa-B4rqnG",
        "outputId": "ad677283-dd4b-4dbd-c4c0-01de1b2717cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of bigrams for the phrase 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of bigrames of information followed by ask\n",
        "print(\"number of bigrams for the phrase\", model.counts[['question']]['ask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM_aujg-50AA",
        "outputId": "a8b15825-4d76-4196-c97b-af2838e6e329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of bigrams for the phrase 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of bigrams for the phrase\", model.counts[['cooking', 'food']]['kitchen'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RvoxSLJr8qb",
        "outputId": "db112e6c-14c8-4fa9-ec36-46c5bbcf33af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of bigrams for the phrase 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the probability score for the word information i.e how likely is the word in our vocanulary\n",
        "# model.score('information') # P('information')\n",
        "model.score(\"information\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGo37no5snOY",
        "outputId": "a86f5e53-e987-4eba-da8a-0bd489e815b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003009170446937041"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probability of 'ask' given 'question'\n",
        "model.score('ask', 'question'.split())  # P('ask'|'question') [question ask]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mAayH0Asnb-",
        "outputId": "94a736cb-ebb7-41fb-830c-37326ccdcbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01282051282051282"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probability of 'is' given 'language'\n",
        "model.score('question', 'ask'.split())  # P('is'|'language') [language is]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGYUIJxE5q0q",
        "outputId": "6a7add0c-fedd-4876-ff27-11c547666bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.017543859649122806"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating text"
      ],
      "metadata": {
        "id": "fJZ63TNvtF0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "metadata": {
        "id": "8Qt1AI0Us2xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    \"\"\"Generate text method  \"\"\"\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ],
      "metadata": {
        "id": "8mzrErT1s2z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(model, 50, random_seed=52)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "rg_czOdjs22o",
        "outputId": "07835734-594f-4350-97b7-62a01b57137c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worked arranged meeting nothing came first obierikas relatives friends began arrive senior people took interest poor clarke two wives ran great alarm pleading sacred week okonkwo man thought things like colours food behaviour liked didnt like would live see desecrated okeke interpreted wisely spirits leaders umuofia asking meet headquarters also'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can generate the above text. It is in form of tokens so it does not make much sense."
      ],
      "metadata": {
        "id": "C4oKZTTsxGiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "KOTliQAc6bDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity=(1/model.score(\"man\"))**(1/3)\n",
        "perplexity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTI0LFWg6RT5",
        "outputId": "eb29e904-11f1-4fdd-b41f-520f5d40fccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.8405554251185805"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perplexity is high at 4.84 which indicates low performance and lower confidence of th model."
      ],
      "metadata": {
        "id": "PUib4ZHEwol5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Reference text from the book\n",
        "reference = [\"school teach young christians\"]\n",
        "\n",
        "# lets see how well the model can predict the word University\n",
        "candidate = \"school teach young \"\n",
        "\n",
        "# Tokenize the reference and candidate sentences\n",
        "reference_tokens = [sentence.split() for sentence in reference]\n",
        "candidate_tokens = candidate.split()\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(reference_tokens, candidate_tokens)\n",
        "print(f'BLEU Score: {bleu_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rhdo_D36RZY",
        "outputId": "24a61fac-481b-4388-903c-d3cebfa9c7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 8.751273976943354e-78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The low BLEU score indicates low performance of the model."
      ],
      "metadata": {
        "id": "lJPpSFr1k_tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training LSTM model"
      ],
      "metadata": {
        "id": "i9eurLnslAAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split this dataset into training, validation, and test sets. This is important for training and evaluating our language model effectively.This split ensures that we have separate datasets for training the model, tuning its parameters, and finally evaluating its performance."
      ],
      "metadata": {
        "id": "nmmZZTX8nEnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the concatenated text into train and test sets\n",
        "train_text, test_text = train_test_split(preprocessed_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the train text into train and validation sets\n",
        "train_text, val_text = train_test_split(train_text, test_size=0.1, random_state=42)\n",
        "\n",
        "# Print the sizes of the train, validation, and test sets\n",
        "print(\"Train set size:\", len(train_text))\n",
        "print(\"Validation set size:\", len(val_text))\n",
        "print(\"Test set size:\", len(test_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yegYAvLb6Rc4",
        "outputId": "1a69a095-cd15-4e14-b20f-3d41e1a8edac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 643877\n",
            "Validation set size: 71542\n",
            "Test set size: 178855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "KWZXhhoa6Rgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m# Define parameters\n",
        "vocab_size = 10000  # Adjust according to your vocabulary size\n",
        "max_sequence_length = 50  # Adjust according to your desired sequence length\n",
        "embedding_dim = 100  # Adjust according to your embedding dimension\n",
        "batch_size = 64\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "mTgr4Acf6Rjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the training data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_text)"
      ],
      "metadata": {
        "id": "6_A2_Arp7kAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text sequences to integer sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "val_sequences = tokenizer.texts_to_sequences(train_text)"
      ],
      "metadata": {
        "id": "Kx7sSu2H7kUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to a fixed length\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_sequence_length, padding='post')"
      ],
      "metadata": {
        "id": "tyXh-A1o69Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for the LSTM model\n",
        "x_train = train_padded[:, :-1]\n",
        "y_train = train_padded[:, -1]\n",
        "x_val = val_padded[:, :-1]\n",
        "y_val = val_padded[:, -1]"
      ],
      "metadata": {
        "id": "3FPptlU369MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "metadata": {
        "id": "NtpDBcPI69Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LnItNohQ69S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGprl_hS8FRM",
        "outputId": "42ad9c98-5aa1-43aa-e512-a8a17e78e925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10061/10061 [==============================] - 134s 13ms/step - loss: 0.0106 - accuracy: 0.9999 - val_loss: 5.9860e-07 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "10061/10061 [==============================] - 167s 17ms/step - loss: 1.1479e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "10061/10061 [==============================] - 172s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "10061/10061 [==============================] - 175s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "10061/10061 [==============================] - 168s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "10061/10061 [==============================] - 168s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10061/10061 [==============================] - 168s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10061/10061 [==============================] - 167s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "10061/10061 [==============================] - 168s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "10061/10061 [==============================] - 168s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ca43f230610>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieves perfect accuracy (100%) and a very low loss on both the training and validation datasets, indicating that it can perfectly predict the outcomes for the given data, this might indicate suggest overfitting."
      ],
      "metadata": {
        "id": "MimCBdXf68iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using perplexity\n",
        "val_loss = model.evaluate(x_val, y_val)[0]\n",
        "perplexity = np.exp(val_loss)\n",
        "print(\"Perplexity:\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTHOsH7LDv1S",
        "outputId": "69253aa1-b129-4d3e-bf00-685aec739d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20122/20122 [==============================] - 93s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Perplexity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perplexity of the model is 1, indicating good performance."
      ],
      "metadata": {
        "id": "DMj6255Xxulq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, max_sequence_length, model, tokenizer):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(50):  # Adjust the number of words to generate\n",
        "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=max_sequence_length-1, padding='pre')\n",
        "        y_pred = np.argmax(model.predict(encoded), axis=-1)\n",
        "        if y_pred[0] == 0:  # Check if predicted index is 0\n",
        "            break  # Exit the loop if predicted index is 0\n",
        "        predicted_word = tokenizer.index_word.get(y_pred[0], '')  # Use get method to handle KeyError\n",
        "        if not predicted_word:\n",
        "            break  # Exit the loop if predicted_word is empty\n",
        "        generated_text += ' ' + predicted_word\n",
        "        seed_text += ' ' + predicted_word\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "uGWtFwoeDwXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using BLEU metric\n",
        "def evaluate_bleu(reference_text, generated_text):\n",
        "    reference_corpus = [word_tokenize(sentence.lower()) for sentence in reference_text]\n",
        "    generated_corpus = [word_tokenize(sentence.lower()) for sentence in generated_text]\n",
        "    return corpus_bleu(reference_corpus, generated_corpus)\n"
      ],
      "metadata": {
        "id": "nYvEpmKXD5hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example evaluation using test set\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
        "x_test = test_padded[:, :-1]\n",
        "y_test = test_padded[:, -1]\n"
      ],
      "metadata": {
        "id": "vPb0vJHaEDkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perplexity and the accuracy indicate that the model is doing well. The low loss also shows that the model is doing well."
      ],
      "metadata": {
        "id": "Sa6pTowR_yUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network\n",
        "\n"
      ],
      "metadata": {
        "id": "nEOYDnnXErW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_fPoQ9-Xz_xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We select the first 100,000 elements of the preprocessed data.\n",
        "partial_text = preprocessed_data[:100000]"
      ],
      "metadata": {
        "id": "d-kH65Kw8Fij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the partial_text into words\n",
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "tokens = tokenizer.tokenize(partial_text.lower())"
      ],
      "metadata": {
        "id": "4ssSXeGG8FlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the unique tokens and create a dictionary mapping tokens to their index\n",
        "unique_tokens = np.unique(tokens)\n",
        "unique_token_index = {token: index for index, token in enumerate(unique_tokens)}"
      ],
      "metadata": {
        "id": "bGCwXbC0E74J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input sequences of n_words tokens and their corresponding next word\n",
        "n_words = 10\n",
        "input_words = []\n",
        "next_word = []\n",
        "\n",
        "for i in range(len(tokens) - n_words):\n",
        "    input_words.append(tokens[i:i + n_words])\n",
        "    next_word.append(tokens[i + n_words])"
      ],
      "metadata": {
        "id": "qVR0LQXFE8G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(input_words), n_words, len(unique_tokens)), dtype=bool)  # for each sample, n input words and then a boolean for each possible next word\n",
        "y = np.zeros((len(next_word), len(unique_tokens)), dtype=bool)  # for each sample a boolean for each possible next word"
      ],
      "metadata": {
        "id": "N-TsZ76pE8TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input sequences and next words into one-hot encoding\n",
        "for i, words in enumerate(input_words):\n",
        "    for j, word in enumerate(words):\n",
        "        X[i, j, unique_token_index[word]] = 1\n",
        "    y[i, unique_token_index[next_word[i]]] = 1"
      ],
      "metadata": {
        "id": "uDIKYXMN8Fn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n"
      ],
      "metadata": {
        "id": "di13IMXlK-ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(unique_tokens)))\n",
        "model.add(Activation(\"softmax\"))"
      ],
      "metadata": {
        "id": "eJozz7S3FOnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and compile the model\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# Train the model and store the training history\n",
        "history = model.fit(X, y, batch_size=128, epochs=10, shuffle=True).history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgVnLMn1FOy-",
        "outputId": "281f8aef-3779-436a-933a-a605b7c0a0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "112/112 [==============================] - 60s 460ms/step - loss: 8.0795 - accuracy: 0.0069\n",
            "Epoch 2/10\n",
            "112/112 [==============================] - 50s 444ms/step - loss: 7.9176 - accuracy: 0.0081\n",
            "Epoch 3/10\n",
            "112/112 [==============================] - 47s 422ms/step - loss: 7.8891 - accuracy: 0.0079\n",
            "Epoch 4/10\n",
            "112/112 [==============================] - 49s 438ms/step - loss: 7.8656 - accuracy: 0.0084\n",
            "Epoch 5/10\n",
            "112/112 [==============================] - 48s 429ms/step - loss: 7.8213 - accuracy: 0.0075\n",
            "Epoch 6/10\n",
            "112/112 [==============================] - 47s 418ms/step - loss: 7.7424 - accuracy: 0.0081\n",
            "Epoch 7/10\n",
            "112/112 [==============================] - 45s 404ms/step - loss: 7.6327 - accuracy: 0.0081\n",
            "Epoch 8/10\n",
            "112/112 [==============================] - 50s 451ms/step - loss: 7.5110 - accuracy: 0.0098\n",
            "Epoch 9/10\n",
            "112/112 [==============================] - 47s 421ms/step - loss: 7.3475 - accuracy: 0.0114\n",
            "Epoch 10/10\n",
            "112/112 [==============================] - 47s 416ms/step - loss: 7.1652 - accuracy: 0.0147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   The loss is high but it is slightly decreasing with each epoch suggesting that the model's predictions are far from the actual values but it is adjusting its weights and improving its predictions over time.\n",
        "   The accuracy is very low but it increases slightly with each iteration meaning that the model's predictions are incorrect for a significant portion of the data but the model is getting better at correctly classifying the data as training progresses."
      ],
      "metadata": {
        "id": "DtD3kJL12l8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the input data X and target labels y with specified batch size, epochs, and shuffling the data\n",
        "# The verbose=0 argument suppresses the output during training, and the .history attribute stores the training history\n",
        "history = model.fit(X, y, batch_size=128, epochs=5, shuffle=True, verbose=0).history\n"
      ],
      "metadata": {
        "id": "Xpw4EtS6FO7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "2vZsLj3GLe2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file named \"text_gen_model2.h5\"\n",
        "model.save(\"text_gen_model2.h5\")\n",
        "\n",
        "# Save the training history to a file named \"history2.p\" using pickle\n",
        "with open(\"history2.p\", \"wb\") as f:\n",
        "    pickle.dump(history, f)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFZKSyshFPB7",
        "outputId": "86d79978-9a8f-4842-a413-5f3b5c92a26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n"
      ],
      "metadata": {
        "id": "AMe5uOzhLtKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"text_gen_model2.h5\")\n",
        "history = pickle.load(open(\"history2.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "3AfQMW49LNhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to predict the next word(s) given an input text and the number of best predictions to return\n",
        "def predict_next_word(input_text, n_best):\n",
        "    input_text = input_text.lower()\n",
        "    X = np.zeros((1, n_words, len(unique_tokens)))\n",
        "    for i, word in enumerate(input_text.split()):\n",
        "        X[0, i, unique_token_index[word]] = 1\n",
        "\n",
        "    predictions = model.predict(X)[0]\n",
        "    return np.argpartition(predictions, -n_best)[-n_best:]"
      ],
      "metadata": {
        "id": "BUw8Fh058Fqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "possible = predict_next_word(\" serious situation \", 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGJvvrgaGYne",
        "outputId": "1a3171ca-8acd-4ea2-9ff3-834ed5e948dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in possible:\n",
        "    print(unique_tokens[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mrZlUp4GY0R",
        "outputId": "f8c836fe-70d5-4ffd-e151-27e80d8f7892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coming\n",
            "mouth\n",
            "bad\n",
            "reject\n",
            "found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above are tokens corresponding to the indices."
      ],
      "metadata": {
        "id": "gND40VB-5HXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function generates words after a certain phrase."
      ],
      "metadata": {
        "id": "aSS0qACx5Scn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate text given an input text, number of words to generate, and creativity level\n",
        "def generate_text(input_text, n_words, creativity=3):\n",
        "    word_sequence = input_text.split()  # Split the input text into a sequence of words\n",
        "    current = 0  # Initialize a variable to keep track of the current position in the word sequence\n",
        "    for _ in range(n_words):\n",
        "        # Create a sub-sequence of n_words from the current position in the word sequence\n",
        "        sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
        "        try:\n",
        "            # Predict the next word(s) based on the sub-sequence and choose one of the predictions\n",
        "            choice = unique_tokens[random.choice(predict_next_word(sub_sequence, creativity))]\n",
        "        except:\n",
        "            # If an exception occurs (e.g., no predictions), choose a random word from the vocabulary\n",
        "            choice = random.choice(unique_tokens)\n",
        "        # Append the chosen word to the word sequence and move the current position by 1\n",
        "        word_sequence.append(choice)\n",
        "        current += 1\n",
        "    # Return the generated text as a single string\n",
        "    return \" \".join(word_sequence)\n"
      ],
      "metadata": {
        "id": "McLVudw0GY4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate some text after a phrase\n",
        "generate_text(\"market place\", 10, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "VzSaeGm6GY7i",
        "outputId": "f40a53ee-9e3c-4922-8632-dc62c60f1a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'market place round young turned might things colleagues close sam called girl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above generated phrase has no meaning, this might be because the text used is preprocessed so it has no stopwords or punctuation or because the model is not doing well."
      ],
      "metadata": {
        "id": "FcviZuZ_5wLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"white man\", 100, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rx9eJw7LGY-o",
        "outputId": "99a37536-1c53-457e-88f9-c1f9ae8d0e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'white man taxi man school completely bad never see sir said sit years days girl girl mm god still second chair college chief look john miss story may right next dinner wait also things thought called first first behind days time quite years english reason called even today light english change clearly bad years quite two car seat lets could perhaps hands alone left back perhaps called found perhaps world general sam girl could left feet four left wonder still said found behind could called mouth see thats might wait car couldnt mouth come said two found humour times last sure give'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the previous text, this also does not seem to make sense, it is just a list of tokens."
      ],
      "metadata": {
        "id": "bIiGFIT-6F3V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-s2pHTjGZBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLqJX2nCGZEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ps9NiWJGZGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3on-5H-nGZJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agw-uvffVaJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXwvJ4vwGXwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qoQEifDmGX30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZvT65ZWGWjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNB_K8cnGWmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AOw3CM3yGWpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HRSbf1sIGWtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4f0TnAL2GWw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-egQHaWPGW0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVYT88hdFNDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gkj8qT1aFNTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EamOgUNAFNh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmPtH5WoVaNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IkcwzYbzVaQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rs54M2hKVaTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "907Ivevg7Qqu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}